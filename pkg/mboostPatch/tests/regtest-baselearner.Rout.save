
R version 3.1.2 (2014-10-31) -- "Pumpkin Helmet"
Copyright (C) 2014 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> library("mboost")
Loading required package: parallel
Loading required package: stabs
This is mboost 2.4-1. See 'package?mboost' and the NEWS file
for a complete list of changes.

> attach(asNamespace("mboost"))
The following objects are masked from package:mboost:

    %+%, %O%, %X%, AUC, AdaExp, Binomial, CoxPH, ExpectReg, FP, Family,
    GammaReg, GaussClass, GaussReg, Gaussian, Gehan, Huber, Hurdle,
    IPCweights, Laplace, Loglog, Lognormal, Multinomial, NBinomial,
    Poisson, PropOdds, QuantReg, Weibull, bbs, blackboost, bmono, bmrf,
    bns, bols, boost_control, brad, brandom, bspatial, bss, btree,
    buser, confint.glmboost, confint.mboost, cv, cvrisk, extract,
    gamboost, glmboost, mboost, mboost_fit, mstop, mstop<-, nuisance,
    risk, selected.mboost, stabsel.mboost, stabsel_parameters.mboost,
    survFit

> library("MASS")
> library("Matrix")
> 
> set.seed(290875)
> 
> ### dgp
> n <- 20000
> xn <- round(runif(n), 3)
> xn[sample(1:n)[1:(n / 100)]] <- NA
> xf <- gl(4, n / 4)
> xf[sample(1:n)[1:(n / 100)]] <- NA
> z1 <- sample(gl(2, n / 2))
> z1[sample(1:n)[1:(n / 100)]] <- NA
> z2 <- round(runif(n), 3)
> z2[sample(1:n)[1:(n / 100)]] <- NA
> w <- rpois(n, lambda = 2)
> y <- 2 * xn + rnorm(n)
> y[is.na(y)] <- rnorm(sum(is.na(y)))
> 
> testfun <- function(m1, m2) {
+     ret <- c(max(abs(coef(m1) - coef(m2))),
+       max(abs(fitted(m1) - fitted(m2)), na.rm = TRUE))
+     if (any(ret > sqrt(.Machine$double.eps)))
+         return(ret)
+ }
> 
> ### numeric x with intercept
> m1 <- lm(y ~ xn, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(xn), w), y)
Warning message:
In bols(xn) : base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### numeric x without intercept
> m1 <- lm(y ~ xn - 1, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(xn, intercept = FALSE), w), y)
Warning messages:
1: In bols(xn, intercept = FALSE) :
  covariates should be (mean-) centered if 'intercept = FALSE'
2: In bols(xn, intercept = FALSE) : base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### factor x with intercept
> m1 <- lm(y ~ xf, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(xf), w), y)
Warning message:
In bols(xf) : base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### factor x without intercept
> tmp <- model.matrix(~ xf)[,-1] ## build model matrix without first row
> mm <- matrix(NA, ncol = ncol(tmp), nrow = length(y))
> mm[!is.na(xf),] <- tmp ## build model matrix with missings
> m1 <- lm(y ~ mm - 1, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(xf, intercept = FALSE), w), y)
Warning message:
In bols(xf, intercept = FALSE) : base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### factor x with "contr.dummy"
> m1 <- lm(y ~ xf - 1, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(xf, contrasts.arg = "contr.dummy"), w), y)
Warning message:
In bols(xf, contrasts.arg = "contr.dummy") :
  base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### contrasts
> m1 <- lm(y ~ xf, weights = w, contrasts = list(xf = "contr.sum"), na.action = na.exclude)
> m2 <- fit(dpp(bols(xf, contrasts.arg = list(xf = "contr.sum")), w), y)
Warning message:
In bols(xf, contrasts.arg = list(xf = "contr.sum")) :
  base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### multiple x
> m1 <- lm(y ~ xn + xf, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(xn, xf), w), y)
Warning message:
In bols(xn, xf) : base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### interaction with binary factor
> xtmp <- (z1 == "2") * xn
> m1 <- lm(y ~ xtmp - 1, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(xn, by = z1, intercept = FALSE), w), y)
Warning messages:
1: In bols(xn, by = z1, intercept = FALSE) :
  covariates should be (mean-) centered if 'intercept = FALSE'
2: In bols(xn, by = z1, intercept = FALSE) :
  base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### interaction with numeric variable
> m1 <- lm(y ~ z2:xn - 1, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(z2, by = xn, intercept = FALSE), w), y)
Warning messages:
1: In bols(z2, by = xn, intercept = FALSE) :
  covariates should be (mean-) centered if 'intercept = FALSE'
2: In bols(z2, by = xn, intercept = FALSE) :
  base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### ridge
> one <- rep(1, n)
> cf1 <- coef(lm.ridge(y ~ one + xn - 1, lambda = 2))
> cf2 <- coef(fit(dpp(bols(one, xn, lambda = 2, intercept = FALSE), rep(1, n)), y))
Warning message:
In bols(one, xn, lambda = 2, intercept = FALSE) :
  base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> max(abs(cf1 - cf2))
[1] 0.001606092
> cf1 <- coef(lm.ridge(y ~ xf - 1, lambda = 2))
> cf2 <- coef(fit(dpp(bols(xf, lambda = 2, intercept = FALSE), rep(1, n)), y))
Warning message:
In bols(xf, lambda = 2, intercept = FALSE) :
  base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> max(abs(cf1 - cf2))
[1] 0.02597228
Warning message:
In cf1 - cf2 :
  longer object length is not a multiple of shorter object length
> 
> ### matrix (here with missing values)
> cf1 <- coef(mod <- lm(y ~ xn + xf * z1 - 1, weights = w, y = TRUE, x = TRUE))
> tX <- mod$x
> tw <- mod$weights
> ty <- mod$y
> cf2 <- coef(fit(dpp(bols(tX), weights = tw), ty))
> stopifnot(max(abs(cf1 - cf2)) < sqrt(.Machine$double.eps))
> 
> ### ridge again with matrix interface
> tX <- matrix(runif(1000), ncol = 10)
> ty <- rnorm(100)
> tw <- rep(1, 100)
> 
> ### compute & check df
> op <- options(mboost_dftraceS = TRUE)
> la <- df2lambda(tX, df = 2, dmat = diag(ncol(tX)), weights = tw)["lambda"]
> truedf <- sum(diag(tX %*% solve(crossprod(tX * tw, tX) + la * diag(ncol(tX))) %*% t(tX * tw)))
> stopifnot(abs(truedf - 2) < sqrt(.Machine$double.eps))
> 
> one <- rep(1, ncol(tX))
> cf1 <- coef(lm.ridge(ty ~ . - 1, data = as.data.frame(tX), lambda = la))
> cf2 <- coef(fit(dpp(bols(tX, df = 2), weights = tw), ty))
> max(abs(cf1 - cf2))
[1] 0.08707475
> # I think bols is better and thus right
> sum((ty - tX %*% cf1)^2) + la * sum(cf1^2)
  lambda 
80.20144 
> sum((ty - tX %*% cf2)^2) + la * sum(cf2^2)
  lambda 
78.29787 
> options(op)
> 
> ### now with other df-definition:
> op <- options(mboost_dftraceS = FALSE)
> la <- df2lambda(tX, df = 2, dmat = diag(ncol(tX)), weights = tw)["lambda"]
> H <- tX %*% solve(crossprod(tX * tw, tX) + la * diag(ncol(tX))) %*% t(tX * tw)
> truedf <- sum(diag(2*H - tcrossprod(H,H)))
> stopifnot(abs(truedf - 2) < sqrt(.Machine$double.eps))
> options(op)
> 
> # check df with weights
> op <- options(mboost_dftraceS = TRUE)
> tw <- rpois(100, 2)
> la <- df2lambda(tX, df = 2, dmat = diag(ncol(tX)), weights = tw)["lambda"]
> truedf <- sum(diag(tX %*% solve(crossprod(tX * tw, tX) + la * diag(ncol(tX))) %*% t(tX * tw)))
> stopifnot(abs(truedf - 2) < sqrt(.Machine$double.eps))
> 
> ### check df2lambda for P-splines (Bug spotted by B. Hofner)
> set.seed(1907)
> x <- runif(100, min = -1, max = 3)
> ## extract lambda from base-learner
> lambda <- bbs(x, df = 4)$dpp(rep(1, length(x)))$df()["lambda"]
> X <- get("X", envir = environment(bbs(x, df = 4)$dpp))
> K <- get("K", envir = environment(bbs(x, df = 4)$dpp))
> truedf <- sum(diag(X %*%  solve(crossprod(X,X) + lambda * K) %*% t(X)))
> stopifnot(abs(truedf - 4) < sqrt(.Machine$double.eps))
> 
> ### check accuracy of df2lambda
> data("bodyfat", package="TH.data")
> diff_df <- matrix(NA, nrow=8, ncol=ncol(bodyfat))
> rownames(diff_df) <- paste("df", 3:10)
> colnames(diff_df) <- names(bodyfat)
> for (i in 3:10){
+     for (j in 1:ncol(bodyfat)){
+         lambda <- bbs(bodyfat[[j]], df = i)$dpp(rep(1, nrow(bodyfat)))$df()["lambda"]
+         diff_df[i-2,j] <- bbs(bodyfat[[j]], lambda = lambda)$dpp(rep(1, nrow(bodyfat)))$df()["df"] - i
+     }
+ }
> stopifnot(all(diff_df < sqrt(.Machine$double.eps)))
> options(op)
> 
> ### componentwise
> cf2 <- coef(fit(dpp(bolscw(cbind(1, xn)), weights = w), y))
> cf1 <- coef(lm(y ~ xn - 1, weights = w))
> stopifnot(max(abs(cf1 - max(cf2))) < sqrt(.Machine$double.eps))
> 
> cf2 <- coef(fit(dpp(bolscw(matrix(xn, nc = 1)), weights = w), y))
> cf1 <- coef(lm(y ~ xn - 1, weights = w))
> stopifnot(max(abs(cf1 - max(cf2))) < sqrt(.Machine$double.eps))
> 
> ### componentwise with matrix
> n <- 200
> m <- 10000
> x <- rnorm(n * m)
> x[abs(x) < 2] <- 0
> X <- Matrix(data = x, ncol = m, nrow = n)
> beta <- rpois(ncol(X), lambda = 1)
> y <- X %*% beta + rnorm(nrow(X))
> w <- rep(1, nrow(X)) ###rpois(nrow(X), lambda = 1)
> f1 <- dpp(bolscw(X), weights = w)$fit
> f1(y)$model
       coef     xselect           p 
   39.20852  3630.00000 10000.00000 
> 
> ### varying coefficients
> x1 <- runif(n, max = 2)
> x2 <- sort(runif(n, max = 2 * pi))
> y <- sin(x2) * x1 + rnorm(n)
> w <- rep(1, n)
> 
> d <- dpp(bbs(x2, by = x1, df = 4), w)
> f <- fit(d, y)
> f2 <- d$predict(list(f), newdata = data.frame(x1 = 1, x2 = x2))
> 
> max(abs(sin(x2) - f2))
[1] 0.7891727
> 
> ### bols and bbs; matrix interfaces
> n <- 10000
> x <- runif(n, min = 0, max = 2*pi)
> y <- sin(x) + rnorm(n, sd = 0.1)
> w <- rpois(n, lambda = 1)
> x[sample(1:n)[1:(n / 100)]] <- NA
> h <- hyper_bbs(data.frame(x = x), vary = "")
> X <- X_bbs(data.frame(x = x), vary = "", h)$X
> f1 <- fit(dpp(bbs(x, df = ncol(X)), w), y)
Warning message:
In bbs(x, df = ncol(X)) : base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> f2 <- fit(dpp(bols(X, df = ncol(X)), w), y)
Warning message:
In bols(X, df = ncol(X)) : base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> stopifnot(max(abs(coef(f1) - coef(f2))) < sqrt(.Machine$double.eps))
> 
> stopifnot(all.equal(get_index(data.frame(x, x)), get_index(X)))
> stopifnot(all.equal(get_index(data.frame(x)), get_index(X)))
> 
> ### check handling of missings for cyclic effects
> h <- hyper_bbs(data.frame(x = x), vary = "", cyclic = TRUE)
> X <- X_bbs(data.frame(x = x), vary = "", h)$X
> stopifnot(all(is.na(X[is.na(x),])))
> stopifnot(all(!is.na(X[!is.na(x),])))
> 
> ### combinations and tensor products of base-learners
> 
> set.seed(29)
> n <- 1000
> x1 <- rnorm(n)
> x2 <- rnorm(n)
> x3 <- rnorm(n)
> f <- gl(4, 25)
> y <- rnorm(n)
> ndf <- data.frame(x1 = x1[1:10], x2 = x2[1:10], f = f[1:10])
> 
> ### spatial
> m1 <- gamboost(y ~ bbs(x1) %X% bbs(x2))
> m2 <- gamboost(y ~ bspatial(x1, x2, df = 16))
> stopifnot(max(abs(predict(m1) - predict(m2))) < sqrt(.Machine$double.eps))
> stopifnot(max(abs(predict(m1, newdata = ndf) - predict(m2, newdata = ndf))) < sqrt(.Machine$double.eps))
> 
> ### spatio-temporal
> m1 <- gamboost(y ~ bbs(x1, knots = 6) %X% bbs(x2, knots = 6) %X% bbs(x3, knots = 6))
> m2 <- gamboost(y ~ (bbs(x1, knots = 6) + bbs(x2, knots = 6)) %X% bbs(x3, knots = 6))
> 
> ### varying numeric
> m1 <- gamboost(y ~ bbs(x1) %X% bols(x2, intercept = FALSE, lambda = 0))
> m2 <- gamboost(y ~ bbs(x1, by = x2, df = 4))
> stopifnot(max(abs(predict(m1) - predict(m2))) < sqrt(.Machine$double.eps))
> stopifnot(max(abs(predict(m1, newdata = ndf) - predict(m2, newdata = ndf))) < sqrt(.Machine$double.eps))
> 
> ### varying factor
> m1 <- gamboost(y ~ bbs(x1) %X% bols(f, df = 5, contrasts.arg = "contr.dummy"))
Warning message:
In data.frame(..., check.names = FALSE) :
  row names were found from a short variable and have been discarded
> coef(m1)
$`"bbs(x1) %X% bols(f, df = 5, contrasts.arg = \\"contr.dummy\\")"`
 [1]  0.2242693758  0.0244549055 -0.0018236656 -0.0649204417 -0.0158616261
 [6] -0.1891022703  0.0633957967 -0.1288712394 -0.3211539316 -0.4271143516
[11]  0.1304389247 -0.1279015955 -0.2844975667 -0.5354756406  0.1377335871
[16]  0.0453968886 -0.0338951732 -0.1953318221 -0.0365952954  0.2189850221
[21]  0.3040001645  0.4485289400 -0.0716853706  0.0631618066  0.3325198072
[26]  0.3788911610  0.2873215520 -0.1578934092  0.0374947404 -0.1538460700
[31]  0.0321145668 -0.1042998008 -0.1475466005 -0.3312423509 -0.3729888541
[36] -0.1410106097 -0.1263566256 -0.0751116331 -0.0884109521 -0.2399923387
[41] -0.0389393947  0.1848036883  0.1656488243  0.0090057253  0.0241351838
[46]  0.0880914017  0.1332083246  0.2231047247  0.0008369162 -0.1344615659
[51]  0.0073865257  0.0724598930  0.0446383409 -0.1022702643  0.0035663694
[56]  0.0550943778 -0.0316890953  0.2015664246 -0.0822295181 -0.0247739018
[61] -0.2539099743  0.0695884531  0.0758519443 -0.4923737237 -0.0674229150
[66]  0.0138897271  0.1275144137 -0.1310550250 -0.0040526037  0.3899574508
[71]  0.3122233938  0.1306996445 -0.0656754995  0.6050839245  0.3881688503
[76]  0.2096929714 -0.0502131903  0.2857074658  0.3854780456  0.1159202913
[81] -0.1884282320 -0.2241611822  0.5098269614 -0.0069895193 -0.1851369840
[86] -0.6265554226  0.4419209852 -0.0189754388 -0.0917695408 -0.5729191166
[91]  0.2786709776 -0.0124807032  0.0007989512 -0.2654733304  0.0868918239
[96] -0.0029929838

attr(,"offset")
[1] 0.03826691
> predict(m1, newdata = ndf)
             [,1]
 [1,]  0.02564341
 [2,]  0.01259759
 [3,]  0.05543313
 [4,] -0.12988977
 [5,] -0.03716691
 [6,] -0.02293550
 [7,] -0.01661927
 [8,] -0.09304843
 [9,]  0.06323357
[10,] -0.13853039
> 
> ### cbind
> m1 <- gamboost(y ~ bols(x1, intercept = FALSE, df = 1) %+%
+                    bols(x2, intercept = FALSE, df = 1))
> m2 <- gamboost(y ~ bols(x1, x2, intercept = FALSE, df = 2))
> stopifnot(max(abs(predict(m1) - predict(m2))) < sqrt(.Machine$double.eps))
> stopifnot(max(abs(predict(m1, newdata = ndf) - predict(m2, newdata = ndf))) < sqrt(.Machine$double.eps))
> 
> ### yeah
> m1 <- gamboost(y ~ (bols(x1, intercept = FALSE, df = 1) %+%
+                     bols(x2, intercept = FALSE, df = 1)) %X% bols(f, df = 4) +
+                     bbs(x1) + bspatial(x1, x2))
Warning message:
In data.frame(..., check.names = FALSE) :
  row names were found from a short variable and have been discarded
> 
> 
> ### test bmono with categorical covariates
> set.seed(781)
> x <- as.ordered(gl(4, 50))
> y <- rnorm(200, mean = rep(c(1,3,2,4), each = 50), sd = 1)
> mod1 <- gamboost(y ~ bols(x))
> diff(coef(mod1)[[1]])
       x2        x3        x4 
 3.773862 -1.313469  1.937386 
> mod2 <- gamboost(y ~ bmono(x, lambda2 = 10^15))
> stopifnot(abs(coef(mod1)[[1]] - coef(mod2)[[1]])[c(1,4)] < 1e-5)
> stopifnot(all(diff(coef(mod2)[[1]]) > - sqrt(.Machine$double.eps)))
> 
> ### test bmono for tensor-product splines
> x1 <- runif(100, min = -2, max = 3)
> x2 <- runif(100, min = -2, max = 3)
> f <- function(x1, x2)
+     0.5 + x1^2 * (x2 + 3)
> y <- rnorm(100, mean = f(x1, x2), sd = 0.5)
> mod1 <- mboost(y ~ bspatial(x1, x2, df = 6, knots = list(x1 = 10, x2 = 5)),
+                control = boost_control(mstop = 50))
> mod21 <- mboost(y ~ bmono(x1, x2, df = 6, knots = list(x1 = 10, x2 = 5),
+                           lambda2 = list(x1 = 10e6, x2 = 0)),
+                 control = boost_control(mstop = 50))
> mod22 <- mboost(y ~ bmono(x1, x2, df = 6, knots = list(x1 = 10, x2 = 5),
+                           lambda2 = list(x1 = 0, x2 = 10e6)),
+                 control = boost_control(mstop = 50))
> 
> diff_order <- 1
> D1 <- kronecker(diff(diag(10 + 3 + 1), differences = diff_order),
+                 diag(5 + 3 + 1))
> D2 <- kronecker(diag(10 + 3 + 1), diff(diag(5 + 3 + 1),
+                               differences = diff_order))
> 
> beta <- coef(mod1)[[1]]
> sum((D1 %*% beta)[D1 %*% beta <= 0])
[1] -117.1438
> sum((D2 %*% beta)[D2 %*% beta <= 0])
[1] 0
> 
> beta <- coef(mod21)[[1]]
> sum((D1 %*% beta)[D1 %*% beta <= 0])
[1] -3.277378e-13
> sum((D2 %*% beta)[D2 %*% beta <= 0])
[1] -0.003959336
> stopifnot(all(D1 %*% beta > - 2e-05))
> 
> beta <- coef(mod22)[[1]]
> sum((D1 %*% beta)[D1 %*% beta <= 0])
[1] -4.849454e-13
> sum((D2 %*% beta)[D2 %*% beta <= 0])
[1] 0
> stopifnot(all(D2 %*% beta > - 2e-05))
> 
> #nd <- expand.grid(sort(x1), sort(x2))
> #names(nd) <- c("x1", "x2")
> #contour(sort(x1), sort(x2),
> #        z = matrix(predict(mod1, newdata = nd), ncol = length(x1)))
> #contour(sort(x1), sort(x2),
> #        z = matrix(predict(mod21, newdata = nd), ncol = length(x1)),
> #        col = "red", add = TRUE)
> #contour(sort(x1), sort(x2),
> #        z = matrix(predict(mod22, newdata = nd), ncol = length(x1)),
> #        col = "green", add = TRUE)
> 
> ### %O%: kronecker product of baselearners for matrix-valued
> ### responses
> x1 <- 1:10/10
> x2 <- 11:17/17
> 
> X1 <- cbind(1, x1, x1^2)
> colnames(X1) <- paste("X1", 1:ncol(X1), sep = "_")
> X2 <- cbind(1, x2, x2^2, x2^3)
> colnames(X2) <- paste("X2", 1:ncol(X2), sep = "_")
> 
> x <- expand.grid(x1, x2)
> colnames(x) <- c("x1", "x2")
> B1 <- with(x, cbind(1, x1, x1^2))
> colnames(B1) <- paste("B1", 1:ncol(B1), sep = "_")
> B2 <- with(x, cbind(1, x2, x2^2, x2^3))
> colnames(B2) <- paste("B2", 1:ncol(B2), sep = "_")
> 
> X <- kronecker(X2, X1)
> w <- rep(1, nrow(X))
> 
> B <- kronecker(matrix(1, ncol = ncol(B2)), B1) *
+      kronecker(B2, matrix(1, ncol = ncol(B1)))
> 
> tol <- 1 / 10000
> stopifnot(max(abs(X - B)) < tol)
> 
> K1 <- diag(ncol(B1))
> K2 <- crossprod(diff(diag(ncol(B2)), diff = 2))
> 
> b1 <- buser(B2, K = diag(ncol(B2))) %X%
+             buser(B1, K = diag(ncol(B1)))
> 
> stopifnot(max(abs(extract(b1, "design") - B)) < tol)
> b1d <- b1$dpp(w)
> 
> b2 <- buser(X1, K = diag(ncol(X1))) %O%
+       buser(X2, K = diag(ncol(X2)))
> b2d <- b2$dpp(w)
> 
> stopifnot(max(abs(get("XtX", environment(b1d$fit)) -
+         get("XtX", environment(b2d$fit)))) < tol)
> 
> y <- runif(nrow(X))
> 
> stopifnot(max(abs(b1d$fit(y)$model - as.vector(b2d$fit(y)$model))) < 1/1000)
> 
> m1 <- b1d$fit(y)
> p1 <- b1d$predict(list(m1, m1))
> 
> m2 <- b2d$fit(y)
> p2 <- b2d$predict(list(m2, m2))
> 
> stopifnot(max(abs(p1 - p2)) < tol)
> 
> 
> x1 <- runif(200)
> x2 <- runif(60)
> y <- rnorm(length(x1) * length(x2))
> d <- expand.grid(x1, x2)
> d$y <- y
> w <- as.vector(rmultinom(1, length(y), rep(1 / length(y), length(y))))
> 
> m1 <- mboost(y ~ bbs(Var2, df = 3, knots = 5) %X%
+                      bbs(Var1, df = 3, knots = 7),
+                      data = d, weights = w)
> m2 <- mboost(y ~ bbs(x1, df = 3, knots = 7)%O%
+                  bbs(x2, df = 3, knots = 5),
+                  weights = w)
> 
> stopifnot(max(abs(fitted(m1) - fitted(m2))) < tol)
> 
> stopifnot(max(abs(coef(m1)[[1]] - coef(m2)[[1]])) < tol)
> 
> stopifnot(max(abs(m1$predict() - m2$predict())) < tol)
> 
> p1 <- predict(m1, newdata = expand.grid(Var1 = c(0.2, 0.5), Var2 = c(0.7, 0.3)))
> p2 <- predict(m2, newdata = data.frame(x1 = c(0.2, 0.5), x2 = c(0.7, 0.3)))
> 
> stopifnot(max(abs(p1 - p2)) < tol)
> 
> ### large data set with ties
> nunique <- 100
> xindex <- sample(1:nunique, 1000000, replace = TRUE)
> x <- runif(nunique)
> y <- rnorm(length(xindex))
> w <- rep.int(1, length(xindex))
> ### brute force computations
> op <- options()
> options(mboost_indexmin = Inf, mboost_useMatrix = FALSE)
> ## data pre-processing
> b1 <- bbs(x[xindex])$dpp(w)
> ## model fitting
> c1 <- b1$fit(y)$model
> options(op)
> ### automatic search for ties, faster
> b2 <- bbs(x[xindex])$dpp(w)
> c2 <- b2$fit(y)$model
> ### manual specification of ties, even faster
> b3 <- bbs(x, index = xindex)$dpp(w)
> c3 <- b3$fit(y)$model
> stopifnot(all.equal(c1, c2))
> stopifnot(all.equal(c1, c3))
> 
> ### new T spline monotonicity
> library("lattice")
> 
> options(mboost_useMatrix = FALSE)
> x <- sort(runif(100, max = 2))
> y <- sin(x) + rnorm(100, sd = .1) + 10
> layout(matrix(1:3, nc = 3))
> plot(x, y)
> m1 <- mboost(y ~ bbs(x))
> lines(x, fitted(m1))
> plot(x, y)
> m2 <- mboost(y ~ bbs(x, constraint = "increasing"))
> lines(x, fitted(m2))
> plot(x, -y)
> m3 <- mboost(I(-y) ~ bbs(x, constraint = "decreasing"))
> lines(x, fitted(m3))
> 
> 
> ### penalty problem -- penalize differences???
> bl <- bbs(x, constraint = "increasing", lambda = 100)
> lines(x, bl$dpp(rep(1, length(y)))$fit(y)$fitted(), col = "red")
> 
> x1 <- seq(from = -3, to = 3, by = .1)
> x2 <- seq(from = 0, to = 2 * pi, by = .1)
> m2 <- sin(x2)
> y <- sapply(m2, function(m) pnorm(x1, mean = m))
> x <- expand.grid(x1 = x1, x2 = x2)
> y <- x$y <- as.vector(y) + runif(nrow(x), min = -.3, max = .3)
> 
> wireframe(y ~ x1 + x2, data = x)
> 
> m1 <- mboost(y ~ bbs(x1) %O% bbs(x2))
> x$p1 <- fitted(m1)
> wireframe(p1 ~ x1 + x2, data = x)
> 
> m2 <- mboost(y ~ bbs(x1, constraint = "increasing", df = 10) %O% bbs(x2))
> x$p2 <- fitted(m2)
> wireframe(p2 ~ x1 + x2, data = x)
> 
> m3 <- mboost(I(-y) ~ bbs(x1, constraint = "decreasing", df = 10) %O% bbs(x2))
> x$p3 <- fitted(m3)
> wireframe(p3 ~ x1 + x2, data = x)
> 
> 
> ### check brandom
> x1 <- rnorm(100)
> x2 <- rnorm(100)
> z1 <- as.factor(sample(1:10, 100, TRUE))
> z2 <- as.factor(sample(1:10, 100, TRUE))
> Zm <- model.matrix(~ z1 - 1)
> Z <- as.data.frame(Zm)
> 
> extract(brandom(z1))
   z11 z12 z13 z14 z15 z16 z17 z18 z19 z110
1    1   0   0   0   0   0   0   0   0    0
2    0   0   0   0   0   0   0   0   1    0
3    0   1   0   0   0   0   0   0   0    0
4    0   0   0   0   0   0   0   1   0    0
6    0   0   1   0   0   0   0   0   0    0
8    0   0   0   0   0   0   1   0   0    0
10   0   0   0   0   0   1   0   0   0    0
14   0   0   0   0   0   0   0   0   0    1
15   0   0   0   1   0   0   0   0   0    0
25   0   0   0   0   1   0   0   0   0    0
attr(,"assign")
 [1] 1 1 1 1 1 1 1 1 1 1
attr(,"contrasts")
attr(,"contrasts")$z1
[1] "contr.dummy"

> extract(brandom(z1, by = x2))
        z11:x2     z12:x2     z13:x2     z14:x2      z15:x2     z16:x2
1    2.1695011  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
2    0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
3    0.0000000  1.4922001  0.0000000  0.0000000  0.00000000  0.0000000
4    0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
5    0.0000000 -0.1066333  0.0000000  0.0000000  0.00000000  0.0000000
6    0.0000000  0.0000000  0.9189804  0.0000000  0.00000000  0.0000000
7    0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
8    0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
9    0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
10   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000 -0.2104969
11   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
12  -0.3235830  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
13   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
14   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
15   0.0000000  0.0000000  0.0000000  0.5972162  0.00000000  0.0000000
16   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
17   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000 -0.8520167
18   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
19   0.0000000  0.0000000  0.0000000 -1.9966841  0.00000000  0.0000000
20   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
21   0.0000000  0.0000000  1.3272031  0.0000000  0.00000000  0.0000000
22   0.0000000  0.0000000  1.0591816  0.0000000  0.00000000  0.0000000
23   0.0000000  0.0000000  0.0000000  0.1030577  0.00000000  0.0000000
24   0.0000000 -1.1900275  0.0000000  0.0000000  0.00000000  0.0000000
25   0.0000000  0.0000000  0.0000000  0.0000000 -0.15108806  0.0000000
26   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
27   0.0000000  0.0000000 -1.1383986  0.0000000  0.00000000  0.0000000
28   0.0000000  0.0000000  0.0000000  0.0000000 -1.16267356  0.0000000
29   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
30   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
31   0.0000000 -1.8740615  0.0000000  0.0000000  0.00000000  0.0000000
32   0.0000000 -1.5200530  0.0000000  0.0000000  0.00000000  0.0000000
33   0.0000000 -0.9286279  0.0000000  0.0000000  0.00000000  0.0000000
34   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
35   0.0000000  0.0000000  0.0000000  0.5714450  0.00000000  0.0000000
36   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
37   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  1.1305174
38   0.0000000  0.0000000 -1.4576627  0.0000000  0.00000000  0.0000000
39   0.0000000  0.0000000  0.0000000  0.0000000  0.25376431  0.0000000
40   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
41   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000 -1.2217863
42   0.0000000  0.0000000  0.0000000  0.0000000 -0.18775971  0.0000000
43  -1.5766083  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
44   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.5037340
45   0.2374643  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
46   0.0000000  0.0000000  0.0000000  0.0000000  1.49349761  0.0000000
47   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
48   0.0000000  0.0000000  0.0000000  0.0000000 -0.09193986  0.0000000
49  -0.4191581  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
50   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  2.0602269
51   0.0554297  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
52   0.0000000  0.0000000 -1.4813092  0.0000000  0.00000000  0.0000000
53   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
54   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
55   0.0000000  0.0000000  0.0000000  0.0000000 -0.53623448  0.0000000
56   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000 -2.2511325
57   0.0000000 -1.0053658  0.0000000  0.0000000  0.00000000  0.0000000
58   1.4773904  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
59   0.0000000  0.0000000  0.0000000 -0.5985393  0.00000000  0.0000000
60   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
61   0.0000000 -0.1748560  0.0000000  0.0000000  0.00000000  0.0000000
62   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
63   0.0000000  0.0000000  0.1346630  0.0000000  0.00000000  0.0000000
64   0.0000000  0.0000000  0.0000000  0.0000000  0.40527666  0.0000000
65   1.2402687  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
66   0.0000000  0.0000000  0.0000000  0.0000000 -0.43270187  0.0000000
67   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
68   0.0000000  0.0000000  0.0000000  1.4185698  0.00000000  0.0000000
69   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
70   0.0000000  0.0000000 -0.3872892  0.0000000  0.00000000  0.0000000
71   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  1.3054256
72   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
73   0.0000000  0.0000000  0.0000000  1.8541271  0.00000000  0.0000000
74   0.0000000  0.0000000 -0.2725320  0.0000000  0.00000000  0.0000000
75   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
76   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
77  -1.1550278  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
78   0.0000000  0.0000000  0.0000000  0.0000000 -1.12138230  0.0000000
79   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
80   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
81   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.7190930
82   0.1301898  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
83   0.0000000  0.0000000  0.0000000  0.0000000 -0.27368892  0.0000000
84   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
85   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
86   0.0000000  0.0000000  0.0000000  0.3563890  0.00000000  0.0000000
87   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
88   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000 -2.1037752
89   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
90   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
91   1.6089437  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
92   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
93   0.0000000  0.0000000  0.0000000  0.0000000 -1.84345887  0.0000000
94   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
95   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
96   0.0000000  0.0000000  0.0000000 -0.6318165  0.00000000  0.0000000
97   0.0000000  0.0000000  0.0000000 -0.5731210  0.00000000  0.0000000
98   0.0000000  0.0000000  0.0000000  0.0000000 -0.86554115  0.0000000
99   0.0000000  0.0000000  0.0000000  0.0000000  0.00000000  0.0000000
100  0.0000000  0.0000000  0.0000000  0.0000000 -1.08570387  0.0000000
         z17:x2     z18:x2     z19:x2     z110:x2
1    0.00000000  0.0000000  0.0000000  0.00000000
2    0.00000000  0.0000000 -0.4308524  0.00000000
3    0.00000000  0.0000000  0.0000000  0.00000000
4    0.00000000  0.1519233  0.0000000  0.00000000
5    0.00000000  0.0000000  0.0000000  0.00000000
6    0.00000000  0.0000000  0.0000000  0.00000000
7    0.00000000 -0.3293577  0.0000000  0.00000000
8    1.21730436  0.0000000  0.0000000  0.00000000
9    0.00000000  0.6173092  0.0000000  0.00000000
10   0.00000000  0.0000000  0.0000000  0.00000000
11   0.00000000  0.0000000  0.3559889  0.00000000
12   0.00000000  0.0000000  0.0000000  0.00000000
13   0.74786772  0.0000000  0.0000000  0.00000000
14   0.00000000  0.0000000  0.0000000  0.14874962
15   0.00000000  0.0000000  0.0000000  0.00000000
16   1.07148359  0.0000000  0.0000000  0.00000000
17   0.00000000  0.0000000  0.0000000  0.00000000
18   0.00000000  0.0000000  0.0000000  0.40176587
19   0.00000000  0.0000000  0.0000000  0.00000000
20   0.00000000 -0.7179130  0.0000000  0.00000000
21   0.00000000  0.0000000  0.0000000  0.00000000
22   0.00000000  0.0000000  0.0000000  0.00000000
23   0.00000000  0.0000000  0.0000000  0.00000000
24   0.00000000  0.0000000  0.0000000  0.00000000
25   0.00000000  0.0000000  0.0000000  0.00000000
26   0.00000000  0.0000000  0.0000000 -0.23262476
27   0.00000000  0.0000000  0.0000000  0.00000000
28   0.00000000  0.0000000  0.0000000  0.00000000
29   0.00000000 -0.7698707  0.0000000  0.00000000
30   1.06109181  0.0000000  0.0000000  0.00000000
31   0.00000000  0.0000000  0.0000000  0.00000000
32   0.00000000  0.0000000  0.0000000  0.00000000
33   0.00000000  0.0000000  0.0000000  0.00000000
34   0.00000000  0.0000000  0.0000000  0.03456592
35   0.00000000  0.0000000  0.0000000  0.00000000
36   0.00000000  0.0000000  0.0000000 -0.23351264
37   0.00000000  0.0000000  0.0000000  0.00000000
38   0.00000000  0.0000000  0.0000000  0.00000000
39   0.00000000  0.0000000  0.0000000  0.00000000
40   0.00000000 -0.1693665  0.0000000  0.00000000
41   0.00000000  0.0000000  0.0000000  0.00000000
42   0.00000000  0.0000000  0.0000000  0.00000000
43   0.00000000  0.0000000  0.0000000  0.00000000
44   0.00000000  0.0000000  0.0000000  0.00000000
45   0.00000000  0.0000000  0.0000000  0.00000000
46   0.00000000  0.0000000  0.0000000  0.00000000
47   0.00000000  0.0000000 -0.3725128  0.00000000
48   0.00000000  0.0000000  0.0000000  0.00000000
49   0.00000000  0.0000000  0.0000000  0.00000000
50   0.00000000  0.0000000  0.0000000  0.00000000
51   0.00000000  0.0000000  0.0000000  0.00000000
52   0.00000000  0.0000000  0.0000000  0.00000000
53   0.00000000 -0.5834946  0.0000000  0.00000000
54   0.00000000  0.0000000  0.0000000  1.68836122
55   0.00000000  0.0000000  0.0000000  0.00000000
56   0.00000000  0.0000000  0.0000000  0.00000000
57   0.00000000  0.0000000  0.0000000  0.00000000
58   0.00000000  0.0000000  0.0000000  0.00000000
59   0.00000000  0.0000000  0.0000000  0.00000000
60   0.00000000  0.0000000 -0.3089929  0.00000000
61   0.00000000  0.0000000  0.0000000  0.00000000
62   0.00000000  0.0000000  0.0000000  0.86122133
63   0.00000000  0.0000000  0.0000000  0.00000000
64   0.00000000  0.0000000  0.0000000  0.00000000
65   0.00000000  0.0000000  0.0000000  0.00000000
66   0.00000000  0.0000000  0.0000000  0.00000000
67   0.00000000  0.0000000  0.0000000  0.49139618
68   0.00000000  0.0000000  0.0000000  0.00000000
69   0.00000000 -0.4005626  0.0000000  0.00000000
70   0.00000000  0.0000000  0.0000000  0.00000000
71   0.00000000  0.0000000  0.0000000  0.00000000
72   0.00000000 -0.3158659  0.0000000  0.00000000
73   0.00000000  0.0000000  0.0000000  0.00000000
74   0.00000000  0.0000000  0.0000000  0.00000000
75   0.00000000  0.0000000  0.0000000  0.21083040
76   0.00000000  0.0000000  0.0000000 -1.06208991
77   0.00000000  0.0000000  0.0000000  0.00000000
78   0.00000000  0.0000000  0.0000000  0.00000000
79   0.00000000  0.6357539  0.0000000  0.00000000
80   0.00000000  0.4886949  0.0000000  0.00000000
81   0.00000000  0.0000000  0.0000000  0.00000000
82   0.00000000  0.0000000  0.0000000  0.00000000
83   0.00000000  0.0000000  0.0000000  0.00000000
84   0.00000000  0.0000000  0.0000000  0.96629051
85   0.00000000  0.1761589  0.0000000  0.00000000
86   0.00000000  0.0000000  0.0000000  0.00000000
87   0.38507695  0.0000000  0.0000000  0.00000000
88   0.00000000  0.0000000  0.0000000  0.00000000
89  -0.68390285  0.0000000  0.0000000  0.00000000
90   0.00000000  0.0000000  0.9601158  0.00000000
91   0.00000000  0.0000000  0.0000000  0.00000000
92   0.00000000 -0.2009500  0.0000000  0.00000000
93   0.00000000  0.0000000  0.0000000  0.00000000
94   1.60482856  0.0000000  0.0000000  0.00000000
95   0.00000000  0.0000000  0.0000000 -1.11173492
96   0.00000000  0.0000000  0.0000000  0.00000000
97   0.00000000  0.0000000  0.0000000  0.00000000
98   0.00000000  0.0000000  0.0000000  0.00000000
99  -0.01762748  0.0000000  0.0000000  0.00000000
100  0.00000000  0.0000000  0.0000000  0.00000000
> extract(brandom(Zm))
    z11 z12 z13 z14 z15 z16 z17 z18 z19 z110
1     1   0   0   0   0   0   0   0   0    0
2     0   0   0   0   0   0   0   0   1    0
3     0   1   0   0   0   0   0   0   0    0
4     0   0   0   0   0   0   0   1   0    0
5     0   1   0   0   0   0   0   0   0    0
6     0   0   1   0   0   0   0   0   0    0
7     0   0   0   0   0   0   0   1   0    0
8     0   0   0   0   0   0   1   0   0    0
9     0   0   0   0   0   0   0   1   0    0
10    0   0   0   0   0   1   0   0   0    0
11    0   0   0   0   0   0   0   0   1    0
12    1   0   0   0   0   0   0   0   0    0
13    0   0   0   0   0   0   1   0   0    0
14    0   0   0   0   0   0   0   0   0    1
15    0   0   0   1   0   0   0   0   0    0
16    0   0   0   0   0   0   1   0   0    0
17    0   0   0   0   0   1   0   0   0    0
18    0   0   0   0   0   0   0   0   0    1
19    0   0   0   1   0   0   0   0   0    0
20    0   0   0   0   0   0   0   1   0    0
21    0   0   1   0   0   0   0   0   0    0
22    0   0   1   0   0   0   0   0   0    0
23    0   0   0   1   0   0   0   0   0    0
24    0   1   0   0   0   0   0   0   0    0
25    0   0   0   0   1   0   0   0   0    0
26    0   0   0   0   0   0   0   0   0    1
27    0   0   1   0   0   0   0   0   0    0
28    0   0   0   0   1   0   0   0   0    0
29    0   0   0   0   0   0   0   1   0    0
30    0   0   0   0   0   0   1   0   0    0
31    0   1   0   0   0   0   0   0   0    0
32    0   1   0   0   0   0   0   0   0    0
33    0   1   0   0   0   0   0   0   0    0
34    0   0   0   0   0   0   0   0   0    1
35    0   0   0   1   0   0   0   0   0    0
36    0   0   0   0   0   0   0   0   0    1
37    0   0   0   0   0   1   0   0   0    0
38    0   0   1   0   0   0   0   0   0    0
39    0   0   0   0   1   0   0   0   0    0
40    0   0   0   0   0   0   0   1   0    0
41    0   0   0   0   0   1   0   0   0    0
42    0   0   0   0   1   0   0   0   0    0
43    1   0   0   0   0   0   0   0   0    0
44    0   0   0   0   0   1   0   0   0    0
45    1   0   0   0   0   0   0   0   0    0
46    0   0   0   0   1   0   0   0   0    0
47    0   0   0   0   0   0   0   0   1    0
48    0   0   0   0   1   0   0   0   0    0
49    1   0   0   0   0   0   0   0   0    0
50    0   0   0   0   0   1   0   0   0    0
51    1   0   0   0   0   0   0   0   0    0
52    0   0   1   0   0   0   0   0   0    0
53    0   0   0   0   0   0   0   1   0    0
54    0   0   0   0   0   0   0   0   0    1
55    0   0   0   0   1   0   0   0   0    0
56    0   0   0   0   0   1   0   0   0    0
57    0   1   0   0   0   0   0   0   0    0
58    1   0   0   0   0   0   0   0   0    0
59    0   0   0   1   0   0   0   0   0    0
60    0   0   0   0   0   0   0   0   1    0
61    0   1   0   0   0   0   0   0   0    0
62    0   0   0   0   0   0   0   0   0    1
63    0   0   1   0   0   0   0   0   0    0
64    0   0   0   0   1   0   0   0   0    0
65    1   0   0   0   0   0   0   0   0    0
66    0   0   0   0   1   0   0   0   0    0
67    0   0   0   0   0   0   0   0   0    1
68    0   0   0   1   0   0   0   0   0    0
69    0   0   0   0   0   0   0   1   0    0
70    0   0   1   0   0   0   0   0   0    0
71    0   0   0   0   0   1   0   0   0    0
72    0   0   0   0   0   0   0   1   0    0
73    0   0   0   1   0   0   0   0   0    0
74    0   0   1   0   0   0   0   0   0    0
75    0   0   0   0   0   0   0   0   0    1
76    0   0   0   0   0   0   0   0   0    1
77    1   0   0   0   0   0   0   0   0    0
78    0   0   0   0   1   0   0   0   0    0
79    0   0   0   0   0   0   0   1   0    0
80    0   0   0   0   0   0   0   1   0    0
81    0   0   0   0   0   1   0   0   0    0
82    1   0   0   0   0   0   0   0   0    0
83    0   0   0   0   1   0   0   0   0    0
84    0   0   0   0   0   0   0   0   0    1
85    0   0   0   0   0   0   0   1   0    0
86    0   0   0   1   0   0   0   0   0    0
87    0   0   0   0   0   0   1   0   0    0
88    0   0   0   0   0   1   0   0   0    0
89    0   0   0   0   0   0   1   0   0    0
90    0   0   0   0   0   0   0   0   1    0
91    1   0   0   0   0   0   0   0   0    0
92    0   0   0   0   0   0   0   1   0    0
93    0   0   0   0   1   0   0   0   0    0
94    0   0   0   0   0   0   1   0   0    0
95    0   0   0   0   0   0   0   0   0    1
96    0   0   0   1   0   0   0   0   0    0
97    0   0   0   1   0   0   0   0   0    0
98    0   0   0   0   1   0   0   0   0    0
99    0   0   0   0   0   0   1   0   0    0
100   0   0   0   0   1   0   0   0   0    0
attr(,"assign")
 [1] 1 1 1 1 1 1 1 1 1 1
attr(,"contrasts")
attr(,"contrasts")$z1
[1] "contr.treatment"

> ## probably non-sense but ok...
> extract(brandom(Z))
    (Intercept) z11 z12 z13 z14 z15 z16 z17 z18 z19 z110
1             1   1   0   0   0   0   0   0   0   0    0
2             1   0   0   0   0   0   0   0   0   1    0
3             1   0   1   0   0   0   0   0   0   0    0
4             1   0   0   0   0   0   0   0   1   0    0
5             1   0   1   0   0   0   0   0   0   0    0
6             1   0   0   1   0   0   0   0   0   0    0
7             1   0   0   0   0   0   0   0   1   0    0
8             1   0   0   0   0   0   0   1   0   0    0
9             1   0   0   0   0   0   0   0   1   0    0
10            1   0   0   0   0   0   1   0   0   0    0
11            1   0   0   0   0   0   0   0   0   1    0
12            1   1   0   0   0   0   0   0   0   0    0
13            1   0   0   0   0   0   0   1   0   0    0
14            1   0   0   0   0   0   0   0   0   0    1
15            1   0   0   0   1   0   0   0   0   0    0
16            1   0   0   0   0   0   0   1   0   0    0
17            1   0   0   0   0   0   1   0   0   0    0
18            1   0   0   0   0   0   0   0   0   0    1
19            1   0   0   0   1   0   0   0   0   0    0
20            1   0   0   0   0   0   0   0   1   0    0
21            1   0   0   1   0   0   0   0   0   0    0
22            1   0   0   1   0   0   0   0   0   0    0
23            1   0   0   0   1   0   0   0   0   0    0
24            1   0   1   0   0   0   0   0   0   0    0
25            1   0   0   0   0   1   0   0   0   0    0
26            1   0   0   0   0   0   0   0   0   0    1
27            1   0   0   1   0   0   0   0   0   0    0
28            1   0   0   0   0   1   0   0   0   0    0
29            1   0   0   0   0   0   0   0   1   0    0
30            1   0   0   0   0   0   0   1   0   0    0
31            1   0   1   0   0   0   0   0   0   0    0
32            1   0   1   0   0   0   0   0   0   0    0
33            1   0   1   0   0   0   0   0   0   0    0
34            1   0   0   0   0   0   0   0   0   0    1
35            1   0   0   0   1   0   0   0   0   0    0
36            1   0   0   0   0   0   0   0   0   0    1
37            1   0   0   0   0   0   1   0   0   0    0
38            1   0   0   1   0   0   0   0   0   0    0
39            1   0   0   0   0   1   0   0   0   0    0
40            1   0   0   0   0   0   0   0   1   0    0
41            1   0   0   0   0   0   1   0   0   0    0
42            1   0   0   0   0   1   0   0   0   0    0
43            1   1   0   0   0   0   0   0   0   0    0
44            1   0   0   0   0   0   1   0   0   0    0
45            1   1   0   0   0   0   0   0   0   0    0
46            1   0   0   0   0   1   0   0   0   0    0
47            1   0   0   0   0   0   0   0   0   1    0
48            1   0   0   0   0   1   0   0   0   0    0
49            1   1   0   0   0   0   0   0   0   0    0
50            1   0   0   0   0   0   1   0   0   0    0
51            1   1   0   0   0   0   0   0   0   0    0
52            1   0   0   1   0   0   0   0   0   0    0
53            1   0   0   0   0   0   0   0   1   0    0
54            1   0   0   0   0   0   0   0   0   0    1
55            1   0   0   0   0   1   0   0   0   0    0
56            1   0   0   0   0   0   1   0   0   0    0
57            1   0   1   0   0   0   0   0   0   0    0
58            1   1   0   0   0   0   0   0   0   0    0
59            1   0   0   0   1   0   0   0   0   0    0
60            1   0   0   0   0   0   0   0   0   1    0
61            1   0   1   0   0   0   0   0   0   0    0
62            1   0   0   0   0   0   0   0   0   0    1
63            1   0   0   1   0   0   0   0   0   0    0
64            1   0   0   0   0   1   0   0   0   0    0
65            1   1   0   0   0   0   0   0   0   0    0
66            1   0   0   0   0   1   0   0   0   0    0
67            1   0   0   0   0   0   0   0   0   0    1
68            1   0   0   0   1   0   0   0   0   0    0
69            1   0   0   0   0   0   0   0   1   0    0
70            1   0   0   1   0   0   0   0   0   0    0
71            1   0   0   0   0   0   1   0   0   0    0
72            1   0   0   0   0   0   0   0   1   0    0
73            1   0   0   0   1   0   0   0   0   0    0
74            1   0   0   1   0   0   0   0   0   0    0
75            1   0   0   0   0   0   0   0   0   0    1
76            1   0   0   0   0   0   0   0   0   0    1
77            1   1   0   0   0   0   0   0   0   0    0
78            1   0   0   0   0   1   0   0   0   0    0
79            1   0   0   0   0   0   0   0   1   0    0
80            1   0   0   0   0   0   0   0   1   0    0
81            1   0   0   0   0   0   1   0   0   0    0
82            1   1   0   0   0   0   0   0   0   0    0
83            1   0   0   0   0   1   0   0   0   0    0
84            1   0   0   0   0   0   0   0   0   0    1
85            1   0   0   0   0   0   0   0   1   0    0
86            1   0   0   0   1   0   0   0   0   0    0
87            1   0   0   0   0   0   0   1   0   0    0
88            1   0   0   0   0   0   1   0   0   0    0
89            1   0   0   0   0   0   0   1   0   0    0
90            1   0   0   0   0   0   0   0   0   1    0
91            1   1   0   0   0   0   0   0   0   0    0
92            1   0   0   0   0   0   0   0   1   0    0
93            1   0   0   0   0   1   0   0   0   0    0
94            1   0   0   0   0   0   0   1   0   0    0
95            1   0   0   0   0   0   0   0   0   0    1
96            1   0   0   0   1   0   0   0   0   0    0
97            1   0   0   0   1   0   0   0   0   0    0
98            1   0   0   0   0   1   0   0   0   0    0
99            1   0   0   0   0   0   0   1   0   0    0
100           1   0   0   0   0   1   0   0   0   0    0
attr(,"assign")
 [1]  0  1  2  3  4  5  6  7  8  9 10
> ## not really useful but might be ok
> extract(brandom(z1, z2))
   z11 z12 z13 z14 z15 z16 z17 z18 z19 z110 z22 z23 z24 z25 z26 z27 z28 z29
1    1   0   0   0   0   0   0   0   0    0   0   0   0   0   0   0   0   0
2    0   0   0   0   0   0   0   0   1    0   0   0   0   0   1   0   0   0
3    0   1   0   0   0   0   0   0   0    0   1   0   0   0   0   0   0   0
4    0   0   0   0   0   0   0   1   0    0   1   0   0   0   0   0   0   0
5    0   1   0   0   0   0   0   0   0    0   0   0   0   1   0   0   0   0
6    0   0   1   0   0   0   0   0   0    0   0   0   0   0   0   0   1   0
7    0   0   0   0   0   0   0   1   0    0   0   1   0   0   0   0   0   0
8    0   0   0   0   0   0   1   0   0    0   0   0   0   0   1   0   0   0
9    0   0   0   0   0   0   0   1   0    0   0   0   0   0   1   0   0   0
10   0   0   0   0   0   1   0   0   0    0   0   0   0   0   0   0   0   0
11   0   0   0   0   0   0   0   0   1    0   0   0   0   1   0   0   0   0
12   1   0   0   0   0   0   0   0   0    0   0   0   0   1   0   0   0   0
13   0   0   0   0   0   0   1   0   0    0   0   1   0   0   0   0   0   0
14   0   0   0   0   0   0   0   0   0    1   0   0   0   0   0   0   0   0
15   0   0   0   1   0   0   0   0   0    0   0   0   0   0   1   0   0   0
16   0   0   0   0   0   0   1   0   0    0   0   0   0   0   0   0   0   0
17   0   0   0   0   0   1   0   0   0    0   0   0   0   0   0   0   0   1
18   0   0   0   0   0   0   0   0   0    1   0   0   0   0   0   0   0   1
19   0   0   0   1   0   0   0   0   0    0   0   1   0   0   0   0   0   0
20   0   0   0   0   0   0   0   1   0    0   0   0   0   0   0   0   0   1
21   0   0   1   0   0   0   0   0   0    0   0   0   0   0   0   0   0   0
22   0   0   1   0   0   0   0   0   0    0   0   0   0   0   0   0   0   1
23   0   0   0   1   0   0   0   0   0    0   0   0   0   0   0   1   0   0
24   0   1   0   0   0   0   0   0   0    0   0   0   0   0   0   0   0   0
25   0   0   0   0   1   0   0   0   0    0   0   0   0   0   0   0   0   0
26   0   0   0   0   0   0   0   0   0    1   0   0   1   0   0   0   0   0
27   0   0   1   0   0   0   0   0   0    0   0   0   0   0   0   0   0   0
28   0   0   0   0   1   0   0   0   0    0   0   0   0   1   0   0   0   0
31   0   1   0   0   0   0   0   0   0    0   0   0   0   0   0   1   0   0
32   0   1   0   0   0   0   0   0   0    0   0   0   0   0   1   0   0   0
34   0   0   0   0   0   0   0   0   0    1   0   0   0   0   0   0   1   0
37   0   0   0   0   0   1   0   0   0    0   0   0   1   0   0   0   0   0
39   0   0   0   0   1   0   0   0   0    0   1   0   0   0   0   0   0   0
41   0   0   0   0   0   1   0   0   0    0   0   0   0   0   1   0   0   0
42   0   0   0   0   1   0   0   0   0    0   0   0   0   0   1   0   0   0
45   1   0   0   0   0   0   0   0   0    0   0   0   1   0   0   0   0   0
46   0   0   0   0   1   0   0   0   0    0   0   0   0   0   0   1   0   0
47   0   0   0   0   0   0   0   0   1    0   0   0   0   0   0   0   1   0
49   1   0   0   0   0   0   0   0   0    0   0   0   0   0   0   1   0   0
50   0   0   0   0   0   1   0   0   0    0   0   1   0   0   0   0   0   0
53   0   0   0   0   0   0   0   1   0    0   0   0   0   0   0   0   0   0
54   0   0   0   0   0   0   0   0   0    1   0   0   0   0   0   0   0   0
55   0   0   0   0   1   0   0   0   0    0   0   0   0   0   0   0   0   1
57   0   1   0   0   0   0   0   0   0    0   0   0   0   0   0   0   1   0
58   1   0   0   0   0   0   0   0   0    0   0   1   0   0   0   0   0   0
60   0   0   0   0   0   0   0   0   1    0   0   0   0   0   0   0   0   1
61   0   1   0   0   0   0   0   0   0    0   0   0   1   0   0   0   0   0
64   0   0   0   0   1   0   0   0   0    0   0   0   0   0   0   0   1   0
66   0   0   0   0   1   0   0   0   0    0   0   1   0   0   0   0   0   0
67   0   0   0   0   0   0   0   0   0    1   0   1   0   0   0   0   0   0
68   0   0   0   1   0   0   0   0   0    0   1   0   0   0   0   0   0   0
69   0   0   0   0   0   0   0   1   0    0   0   0   0   0   0   1   0   0
70   0   0   1   0   0   0   0   0   0    0   0   0   0   0   0   1   0   0
73   0   0   0   1   0   0   0   0   0    0   0   0   0   0   0   0   1   0
78   0   0   0   0   1   0   0   0   0    0   0   0   1   0   0   0   0   0
80   0   0   0   0   0   0   0   1   0    0   0   0   0   0   0   0   0   0
83   0   0   0   0   1   0   0   0   0    0   0   0   0   0   0   0   0   0
85   0   0   0   0   0   0   0   1   0    0   0   0   0   0   0   0   1   0
86   0   0   0   1   0   0   0   0   0    0   0   0   0   0   0   0   0   0
87   0   0   0   0   0   0   1   0   0    0   0   0   0   0   0   1   0   0
90   0   0   0   0   0   0   0   0   1    0   0   0   0   0   0   0   0   0
97   0   0   0   1   0   0   0   0   0    0   0   0   0   1   0   0   0   0
99   0   0   0   0   0   0   1   0   0    0   0   0   0   0   0   0   0   0
   z210
1     0
2     0
3     0
4     0
5     0
6     0
7     0
8     0
9     0
10    0
11    0
12    0
13    0
14    1
15    0
16    0
17    0
18    0
19    0
20    0
21    1
22    0
23    0
24    0
25    1
26    0
27    0
28    0
31    0
32    0
34    0
37    0
39    0
41    0
42    0
45    0
46    0
47    0
49    0
50    0
53    0
54    0
55    0
57    0
58    0
60    0
61    0
64    0
66    0
67    0
68    0
69    0
70    0
73    0
78    0
80    1
83    0
85    0
86    1
87    0
90    1
97    0
99    1
attr(,"assign")
 [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2
attr(,"contrasts")
attr(,"contrasts")$z1
[1] "contr.dummy"

attr(,"contrasts")$z2
[1] "contr.dummy"

> ## should throw an error
> try(extract(brandom(x1, by = x2, intercept = FALSE)))
Error in brandom(x1, by = x2, intercept = FALSE) : 
  '...' must be a factor or design matrix in 'brandom'
> 
> ## check if one can specify either df or lambda
> round(extract(brandom(z1, df = 3)$dpp(rep(1, 100)), what = "lambda"), 2)
lambda 
 50.39 
> round(extract(brandom(z1, df = 3)$dpp(rep(1, 100)), what = "df"), 2)
df 
 3 
> round(extract(brandom(z1, lambda = 50.39)$dpp(rep(1, 100)), what = "lambda"), 2)
lambda 
 50.39 
> round(extract(brandom(z1, lambda = 50.39)$dpp(rep(1, 100)), what = "df"), 2)
df 
 3 
> 
> proc.time()
   user  system elapsed 
 33.789   1.594  35.922 
